<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>AMAC + VR</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="Site Description Here">
        <link href="css/bootstrap.css" rel="stylesheet" type="text/css" media="all" />
        <link href="css/stack-interface.css" rel="stylesheet" type="text/css" media="all" />
        <link href="css/socicon.css" rel="stylesheet" type="text/css" media="all" />
        <link href="css/lightbox.min.css" rel="stylesheet" type="text/css" media="all" />
        <link href="css/flickity.css" rel="stylesheet" type="text/css" media="all" />
        <link href="css/iconsmind.css" rel="stylesheet" type="text/css" media="all" />
        <link href="css/jquery.steps.css" rel="stylesheet" type="text/css" media="all" />
        <link href="css/theme.css" rel="stylesheet" type="text/css" media="all" />
        <link href="css/custom.css" rel="stylesheet" type="text/css" media="all" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:200,300,400,400i,500,600,700%7CMerriweather:300,300i" rel="stylesheet">
        <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
        <meta charset="UTF-8">
    </head>
    <body class=" ">
        <a id="start"></a>

        <div class="nav-container ">
            <nav id="menu1" class="bar bar--sm bar-1 hidden-xs">
                <div class="container">
                    <div class="row"  style="margin-top: 10px">
                        <div class="col-lg-4 col-md-6 hidden-xs">
                            <div class="bar__module">
                                <h4 style="margin-bottom: 15px !important"><b>AMAC + VR</b></h4>
                            </div>
                            <!--end module-->
                        </div>
                        <div class="col-lg-8 col-md-6 text-right text-left-xs text-left-sm">
                            <div class="bar__module">
                                <ul class="menu-horizontal text-left">
                                  <li><a class="inner-link" href="#overview">Overview</a></li>
                                  <li><a class="inner-link" href="#introduction">Introduction</a></li>
                                  <li><a class="inner-link" href="#autonomous-car">Autonomous Car</a></li>
                                  <li><a class="inner-link" href="#vr">VR</a></li>
                                  <li><a class="inner-link" href="#conclusion">Conclusion</a></li>
                                  <li><a class="inner-link" href="#team">Team</a></li>
                                </ul>
                            </div>
                            <!--end module-->
                        </div>
                    </div>
                    <!--end of row-->
                </div>
                <!--end of container-->
            </nav>
            <!--end bar-->
        </div>

        <div class="main-container">
           <section class="cover height-60 imagebg text-center" data-overlay="4">
                <div class="background-image-holder">
                    <img alt="background" src="img/lidar.png" />
                </div>
                <div class="container pos-vertical-center">
                    <div class="row">
                        <div class="col-md-8 col-lg-7">
                            <h1>
                                Robots for Collaborative Tasks
                            </h1>
                            <p class="lead">
                                AMAC Autonomous Car <br> ‚ÄúBerkeley Goggles‚Äù VR Robotic Teleoperation system
                            </p>
                        </div>
                    </div>
                    <!--end of row-->
                </div>
                <!--end of container-->
            </section>

            <section id="overview">
                <div class="container">
                    <div class="row justify-content-center">
                        <div class="col-md-10 col-lg-8">
                            <h2 class="text-center">Overview and Results Video</h2>
                            <p class="lead">
                            </p>
                        </div>
                    </div>
                    <!--end of row-->
                </div>
                <!--end of container-->
            </section>
            <section>
                <div class="container">
                    <div class="row justify-content-center">
                        <div class="col-md-12 col-lg-10 text-center">
                            <div class="video-cover box-shadow border--round">
                                <div class="background-image-holder">
                                    <img alt="image" src="img/work-single-1.jpg" />
                                </div>
                                <div class="video-play-icon"></div>
                                <iframe data-src="https://www.youtube.com/embed/UFQFUFGn_38" allowfullscreen="allowfullscreen"></iframe>
                            </div>
                            <!--end video cover-->
                            <span>In this video we demonstrate our autonomous car and virtual robot collaborating together.</span>
                        </div>
                    </div>
                    <!--end of row-->
                </div>
                <!--end of container-->
            </section>
            <section class="imagebg" class="pos-vertical-center" data-gradient-bg="#328A2E,#256F5C,#29506D,#047BE6" id="introduction">
              <div class="text-center">
                        <h1>Introduction</h1>
              </div>
            </section>
            <section>
              <div class="container">
	             <div class="row">
		               <div class="col-md-12">
                      <p>
                        The goal of this project is to combine two key elements through Virtual Reality: LIDAR data from an autonomous RC car and using a custom robotic manipulator system for sensing the environment and manipulation.
                       </p>
                      <p>
                        This project is unique because it requires integration of multiple sensors onto a vehicle that can enable autonomous decision-making is a modern-challenge. Creating an interface for remote robotic control is an open research problem which when done correctly, could enable remote blue-collar work around the world. Lastly, the interaction between two robots that are not directly communicating is another problem that this project addresses.
                      </p>
                      <p>
                        A direct use case of this system is in hospitals, where the autonomous RC car can deliver medications or and the Blue Robot can be teleoperated or trained to pick up the medicine and administer it to a patient in a hospital room. Another use case is in a manufacturing facility, where the car can deliver parts to various parts of the factory, and the robot can then be manipulated to use these parts in the assembly line (great for low-volume, high-mix manufacturing).
                      </p>

                      <p>
                        Our overarching goal is to integrate an autonomous vehicle and the ‚Äúblue‚Äù robotic manipulator in the same environment so that they can collaboratively solve an assigned task that demonstrates the abilities of each system to their full degree. The design criteria for the car includes having robust components for the car in addition to features such as path planning and object detection, following, and avoidance.
                      </p>
                   </div>
	             </div><!--end of row-->
              </div><!--end of container-->
            </section>

          </section>
          <section class="imagebg" class="pos-vertical-center" data-gradient-bg="#328A2E,#256F5C,#29506D,#047BE6" id="autonomous-car">
            <div class="text-center">
                      <h1>AMAC Autonomous Car</h1>

            </div>
          </section>
          <section class="switchable feature-large bg--secondary">
              <div class="container">
                  <div class="row justify-content-around">
                      <div class="col-md-6">
                          <img alt="Image" class="border--round box-shadow-wide" src="img/car_model2.png" />
                      </div>
                      <div class="col-md-6 col-lg-5">
                          <div class="switchable__text">
                              <h2>AMAC Car</h2>
                              <p class="lead">
                                An autonomous car, built from the ground up.
                              </p>
                                <p class="lead">All of our code for the AMAC car is publicly available on our github at: <a href="https://github.com/AMABerkeley/" target="_blank">AMAC Github</a>.</p>
                              <p>
                                <b>Hardware included:</b> <br>
                                Ouster LIDAR <br>
                                IMU <br>
                                3 RGB Camera Assembly <br>
                                Intel NUC <br>
                                BLDC Motor <br>
                                ESC - Motor Controller <br>
                              </p>
                              <!--<a href="#">Learn More &raquo;</a>-->
                          </div>
                      </div>
                  </div>
                  <!--end of row-->
              </div>
              <!--end of container-->
          </section>
          <section>
            <div class="container">
             <div class="row">
                 <div class="col-md-12">

                   <h2>AMAC Design Criteria: </h2>
                      <!-- Path planning, object detection, following, and avoidance -->
                    </p>

                    <p>
                      First, we had to consider what we could actually accomplish in a single semester.
                      For this car, we prioritized sensors that would aid in environment mapping, localization, and nearby object detection, so we chose a LiDAR, Inertial Navigation System (INS), and 3 cameras. Most of the RC car components were purchased off the shelf so we could focus on sensor, planning, and actuation. Ultrasonic sensors were also considered for low-proximity object detection for emergency stopping and planning.
                      A demonstration of this criteria includes the vehicle driving to a location near the robot, the VR teleoperated robot picking up an item and placing it on the car for transport, and the car driving away with this item.


                      </p>

                    <p>
                      These design choices were crucial in meeting our initial goals and design criteria as we wanted each robot to mimic real engineering situations and could be scalable as such. The computer, an Intel NUC, was essential for processing all of the sensor data into vehicle control commands. The battery (7 cell 29.4V, 6000Mah, 144Wh) was selected to account for the heavy power demand from the computer, drivetrain, and sensors.
                      The design criteria for the car includes having robust components for the car in addition to features such as path planning and object detection, following, and avoidance.
                      These design decisions were crucial in best exhibiting that our project would be as robust, durable, and efficient as possible.
                    </p>


                 </div>
             </div><!--end of row-->
            </div><!--end of container-->
          </section>

          <section class="switchable feature-large bg--secondary">
              <div class="container">
                  <div class="row justify-content-around">
                      <div class="col-md-6">
                          <img alt="Image" class="border--round box-shadow-wide" src="img/car_diagram.png" />
                      </div>
                      <div class="col-md-6 col-lg-5">
                          <div class="switchable__text">
                              <h2>AMAC Operations Diagram</h2>
                              <p class="lead">
                                <!-- An autonomous car, built from the ground up. -->
                                Every sensor integration - both hardware and software - were built from the ground up. 
                              </p>

                              <!--<a href="#">Learn More &raquo;</a>-->
                          </div>
                      </div>
                  </div>
                  <!--end of row-->
              </div>
              <!--end of container-->
          </section>
          <section>
            <div class="container">
             <div class="row">
                 <div class="col-md-12">
                    <h2>AMAC Implementation</h2>

                    <div class="center">
                      <img alt="Image" src="img/car_diagram2.png"/>
                    </div>
                    <p>
                      The car was built from the ground up. We took a traditional 1/8th RC Car and stripped it down. We then added wiring for drive by wire capability, 3D printed a sensor chassis, created a power budget and wired everything to one battery using multiple buck-boast converters.
                    </p>
                    <div class="row">
                      <div class="column">
                        <img alt="Image" src="img/electronics.png" width="500" height="300" />
                      </div>
                      <div class="column">
                        <img alt="Image" src="img/3dprint.png" width="500" height="300" />
                      </div>
                    </div>
                    <p>
                      A ‚Äúdriver‚Äù was necessary to communicate with the LiDAR via Ethernet. This ‚Äú3DPointCloud‚Äù is converted by projecting 3D points into a 2D LaserScan structure (defined by a pre-existing Velodyne LiDAR package), which will be used for creating a map.
                    </p>

                    <p>
                      In parallel, the Inertial Navigation System (INS) publishes a unique message type ‚Äúins‚Äù which is converted to an odometry message ‚Äúodom‚Äù.
                    </p>

                    <p>
                      The ROS package, gmapping, uses 5DOF slam with the 2D laserscan and the odometry message to publish data in an occupancy grid and to determine relative position for our environment. Our system is also capable of 6DOF slam.
                    </p>
                    <div class="center">
                      <img alt="Image" src="img/5dof.jpg" style="width:60%"/>
                    </div>
                    <p>
                      The navigation stack uses a TEB planner to develop a local and global cost map which utilizes the occupancy grid and ‚Äútf‚Äù transform. The TEB planner uses a simple Ackermann steering model. When selecting a navigation goal, it calls on the local and global planners to find a path in planner, which then publishes waypoints (in the form of a Twist) to the cmd_vel topic. This Twist is then converted to PWM values for our steering and drive motors. These values are then subscribed to with the drive-by-wire computer communication node to send the necessary signals to the ESC and servo.
                    </p>
                    <div class="center">
                      <img alt="Image" src="img/Ackermann_turning.png" style="width:70%"/>
                    </div>

                    <p>
                      Alternatively, the car can also be controlled via object detection through the front cameras. When running the object detection scheme, the raw camera data is published to usb_cam and the object detection package determines where in the frame the object exists. Depending on the object seen, the car will perform different actions. We trained 3 different signs, Reverse, Follow, and Stop. This information also publishes to the ‚Äúcmd_vel‚Äù topic as a Twist and is processed by the Control Interface the same way messages from the Navigation stack are processed.
                    </p>
                    <div class="row">
                      <div class="column">
                        <img alt="Image" src="img/multiple_cameras.jpg" width="500" height="300" />
                      </div>
                      <div class="column">
                        <img alt="Image" src="img/go_sign-1.png" width="500" height="300" />
                      </div>
                    </div>
                    <p>
                      A URDF model of the car was created from a 3D CAD in Solidworks. All coordinate frames were connected appropriately in the tf tree. The ‚Äútf‚Äù and ‚Äúmap‚Äù topics were subscribed to by the navigation stack.
                    </p>
                    <div class="row">
                      <div class="column">
                        <img alt="Image" src="img/urdf_tree.png" width="550" height="300" />
                      </div>
                      <div class="column">
                        <img alt="Image" src="img/urdf_rviz.png" width="400" height="300" />
                      </div>
                    </div>
                 </div>
             </div><!--end of row-->
            </div><!--end of container-->
          </section>
          <section class="switchable feature-large bg--secondary">
              <div class="container">
                <h2>AMAC Results</h2>
                <p>
                  The car demonstrated most of the functionality we were hoping to accomplish. The car was able to generate a map of its environment and localize itself using the various sensors we incorporated. We were then able to select a destination for the car to drive to and it would plan a trajectory to the destination and drive there. In addition, the car was able to identify pre-trained objects with computer vision when placed in front of the car and act accordingly.
                </p>

                  <!--end of row-->
              </div>]=
          <div align="center">
            <video  autoplay loop style="width:70%">
              <source src="img/car_driving.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video>
          </div>
              <!--end of container-->
          </section>




          <section class="imagebg" class="pos-vertical-center" data-gradient-bg="#328A2E,#256F5C,#29506D,#047BE6" id="vr">
            <div class="text-center">
                      <h1>VR Teleoperation System</h1>
            </div>
          </section>

          <section class="switchable feature-large bg--secondary">
              <div class="container">
                  <div class="row justify-content-around">
                      <div class="col-md-6">
                          <img alt="Image" class="border--round box-shadow-wide" src="img/blue_arm.jpg" />
                      </div>
                      <div class="col-md-6 col-lg-5">
                          <div class="switchable__text">
                              <h2>Blue VR Teleoperation System</h2>
                              <p class="lead">
                                Intuitive, real-time, visual robotic manipulation.
                              </p>
                              <p>
                                <b>Hardware included:</b> <br>
                                "Blue" Robotic Manipulator <br>
                                Intel RealSense D435 Depth Camera <br>
                                HTC Vive VR System <br>
                              </p>
                              <!--<a href="#">Learn More &raquo;</a>-->
                          </div>
                      </div>
                  </div>
                  <!--end of row-->
              </div>
              <!--end of container-->
          </section>

          <section>
            <div class="container">
             <div class="row">
                 <div class="col-md-12">

                   <h2> VR Teleoperation Design: </h2>



                    <p>
                      <b>Design Criteria</b> <br>
                      Successfully view robotic manipulator‚Äôs environment via depth map stream. <br>
                      Communicate VR 6DOF poses to IK solver on robotic device. <br>
                      Record teleoperation movements and play movement back. <br>
                    </p>



                    <p>
                      <b>Design Overview</b> <br>
                      For the VR teleoperation system, the design prioritized an interface which both stayed within the computational limits of the VR system and also gave the teleoperator the most intuitive, powerful interface. For example, multiple depth-sensing cameras could not be incorporated to make the VR vision of the environment better due to the computational limits of the system we were building on.
                    </p>

                    <p>
                      Additionally, we sought to design the human-robot interaction system such that the operator could easily control the orientation of the robot gripper, regardless of their wrist‚Äôs true orientation. From this design requirement, we formulated that the user should be able to ‚Äúclutch‚Äù the robot graspers and easily disengage from the control of the graspers. This is seen in the visualization below.
                    </p>

                    <p>
                      The design choices for the VR teleoperation system heavily impact the graphical efficiency of the system, the functionality and usability of the remote interaction, and its successful communication with the robotic manipulator. The ROS# was selected as an interface to ROS through Unity so that the VR interface can be easily maintainable for future users and developers. Additionally, the VR scene was designed such that minimal changes should be needed for other robotic or VR systems.
                    </p>

                    <p>
                      These design decisions were crucial in best exhibiting that our project would be as robust, durable, and efficient as possible.
                    </p>


                 </div>
             </div><!--end of row-->
            </div><!--end of container-->
          </section>

          <section class="switchable feature-large bg--secondary">
              <div class="container">
                  <div class="row justify-content-around">
                      <div class="col-md-6">
                          <img alt="Image" class="border--round box-shadow-wide" src="img/vr_diagram.png" />
                      </div>
                      <div class="col-md-6 col-lg-5">
                          <div class="switchable__text">
                              <h2>The VR Teleoperation Operations Diagram</h2>
                              <p class="lead">
                                A VR robotic teleoperation system, built from the ground up.
                              </p>

                              <!--<a href="#">Learn More &raquo;</a>-->
                          </div>
                      </div>
                  </div>
                  <!--end of row-->
              </div>
              <!--end of container-->
          </section>


          <section>
            <div class="container">
             <div class="row">
                 <div class="col-md-12">

                    <h2>VR Implementation</h2>
                    <p>
                      The VR Interface begins in the Unity editor where it a script initializes the transforms in the scene. This begins by using a model of the robot to initialize the base and end effector transforms of the robot in the position that it begins in. On the robot side, the blue_teleop launch file is launched to begin subscribing to the VR interface. This also puts the robot in a pre-set position that the VR interface can then begin from. Once the robot is launched, the VR interface is launched. The VR interface subscribes to joint_states and publishes end effector poses in addition to a float value for the left and right grippers. With this simple communication interface, enough information is transferred to enable an entirely new, intuitive interface for the robotic manipulator.
                    </p>
                    <p>
                      In the VR scene, the user can grab 2 virtual gripper models using the trigger on the VR controller. When this is done, it makes the gripper model become a rigid body with their controller and these poses are sent in the correct frame to the subscriber on the robot. With the joint_states subscription, the VR interface also visualizes the URDF of the ‚Äúblue‚Äù robot in real time as the user moves the controller. The depth camera on the robot also is aligned with the correct transform such that the URDF seemingly reaches into the depth stream and is able to manipulate things. There also exists a button to record robot actions. This works by programmatically recording the poses, in their correct frame, being sent to the robot for the duration of the recording. This sequence of poses can then be played back for specific tasks.
                    </p>

                    <br>

                    <p>
                      <b>How the end-to-end AMAC and VR system works:</b>
                    </p>

                    <ol>
                      <li>The user launches the car and selects a point as the destination for the car to drive to. This would typically be a location within the Blue robot‚Äôs workspace. </li>
                      <li>The car will plan it path to the destination based on the map generated by its various sensors, avoiding obstacles that may impede its path.</li>
                      <li>The Blue robot will execute a trained process of delivering a payload to the car.</li>
                      <li>Upon receiving the payload, the car will then travel to its next destination.</li>

                    </ol>

                    <br>


                 </div>
             </div><!--end of row-->
            </div><!--end of container-->
          </section>

          <section class="switchable feature-large bg--secondary">
              <div class="container">
                <h2>VR Results</h2>
                <p>
              The VR Teleoperation system performs as expected. The user can first connect to the robotic manipulator by inputting its network information. This will set up the aforementioned subscribers and publishers. Once the rosnode of robotic manipulator in launched, the URDF will begin to be visualized in VR and grabbing the manipulator in the VR simulation enables the user to move the manipulator‚Äôs grippers in real-time. There is also the option to save a sequence of poses from one timestamp to another and play this recording back.
                            </p>
                  <!--end of row-->
              </div>
              <!--end of container-->
          </section>

        </section>

        <section class="switchable feature-large bg--secondary">
            <div class="container">
                <div class="row justify-content-around">

                    <div class="col-md-8">
                        <img alt="Image" class="border--round box-shadow-wide" src="img/interface.png" />
                    </div>


                </div>
                <!--end of row-->
            </div>
            <!--end of container-->
        </section>






       <section class="imagebg" class="pos-vertical-center" data-gradient-bg="#328A2E,#256F5C,#29506D,#047BE6" id="conclusion">
         <div class="text-center">
                   <h1>Conclusion</h1>
         </div>
       </section>

       <section>
         <div class="container">
          <div class="row">
              <div class="col-md-12">

                <p>
                   We were excited to see each part of this project come together and meet our design goals. The autonomous RC car was able to successfully path plan, identify objects, and actuate depending on these. Furthermore, the VR teleoperation system was able to successfully visualize objects in front of the robotic manipulator and real-time send desired gripper poses and grasping values to the robot‚Äôs ROS system. 
                </p>
                <p>
                  One of the biggest initial challenges was wiring the entire circuit on the car. There are many different electrical components on board the car and it is challenging to keep track of all the wires! Another challenge was getting the drive-by-wire computer to interface with ROS. It took a long time before we were able to even drive the car by wire. Another challenge was getting our occupancy grid to update fast enough for the car to properly avoid obstacles. 
                </p>
                <p>
                  Our navigation package assumed a differential robot rather than a car-like robot like our system, and thus published command velocities for a robot which had different constraints than our own. We attempted to hack this by massaging these velocities to obey the constraints of our system. We utilized a kinematic bicycle model to do this. Unfortunately, this did not work as well as we had hoped and with more time, we would utilize a planner that uses car-like dynamics to plan a trajectory. Several improvements could be made to the visualization of the data we are collecting. An integration of the LiDAR point cloud with the VR environment would be a useful next step. Additionally, the URDF could be made such that the front two wheels are connected through by the ‚ÄúAckermann Steering‚Äù mechanism. The LiDAR point cloud could also be overlayed with camera information to assist with object detection and avoidance. 
                </p>
                <p>
                  With additional time, the VR teleoperation system could be extended to generalize to all robotic manipulator systems. With some slight redesigns, the project could be open sourced as a general interface for robotic manipulators and the user of the project would simply have to set up the VR interface‚Äôs URDF, network, and other minor parameters. Additionally, the system could be extended to incorporate multiple camera system for greater visual flexibility for the operator. This work will be done as the project is continued next semester in the same research lab. 
                 </p>
                 <p>
                  All in all we learned a tremendous amount from this project and will continue research on it inn the coming semesters. All of our code for the AMAC car is publicly available on our github at:  <a href="https://github.com/AMABerkeley/" target="_blank">AMAC Github</a>.
                </p>


              </div>
          </div><!--end of row-->
         </div><!--end of container-->
       </section>



           <section class="imagebg" class="pos-vertical-center" data-gradient-bg="#328A2E,#256F5C,#29506D,#047BE6" id="team">
             <div class="text-center">
                       <h1>Team</h1>
             </div>
           </section>
           <section class="text-center bg--secondary">
             <div class="container">
                 <div class="row">
                     <div class="col-md-3">
                         <div class="feature feature-8">
                             <img alt="Image" src="img/bryce.png" />
                             <h5>Bryce Schmidtchen</h5>
                             <span><b>Major:</b> EECS <br>
                              <b>Experience:</b> Strong background in graphics, SLAM, computer vision, and AR/VR. Worked on these technologies at NASA, Apple, and Magic Leap as well as under Dr. Allen Yang / Shankar Sastry in the Center for Augmented Cognition.
                              </span>
                         </div>
                     </div>
                     <div class="col-md-3">
                         <div class="feature feature-8">
                             <img alt="Image" src="img/travis.png" />
                             <h5>Travis Brashears</h5>
                             <span><b>Major:</b> Engineering Physics <br> <b>Experience:</b> Space + Lasers üíïüöÄ
</span>
                         </div>
                     </div>
                     <div class="col-md-3">
                         <div class="feature feature-8">
                             <img alt="Image" src="img/carl.png" />
                             <h5>Carl Canteenwala</h5>
                             <span><b>Major:</b> Mechanical Engineering, Minor in EECS <br>
                               <b>Experience: </b>Mechatronic systems such as a monowheel and actuated prosthetic hand. Design experience in Mechanical Systems Control lab and a variety of control theory classes

                             </span>
                         </div>
                     </div>
                     <div class="col-md-3">
                         <div class="feature feature-8">
                             <img alt="Image" src="img/andy2.jpg" />
                             <h5>Andy Meyers</h5>
                             <span><b>Major: </b>Mechanical Engineering <br> <b>Experience: </b> has worked on a ‚Äúbiomimetic lizard robot‚Äù, previous product design experience at Apple
</span>
                         </div>
                     </div>
                 </div>
                 <!--end of row-->
             </div>
             <!--end of container-->
        </div>
        <!--<div class="loader"></div>-->
        <a class="back-to-top inner-link" href="#start" data-scroll-class="100vh:active">
            <i class="stack-interface stack-up-open-big"></i>
        </a>
        <script src="js/jquery-3.1.1.min.js"></script>
        <script src="js/flickity.min.js"></script>
        <script src="js/easypiechart.min.js"></script>
        <script src="js/parallax.js"></script>
        <script src="js/typed.min.js"></script>
        <script src="js/datepicker.js"></script>
        <script src="js/isotope.min.js"></script>
        <script src="js/ytplayer.min.js"></script>
        <script src="js/lightbox.min.js"></script>
        <script src="js/granim.min.js"></script>
        <script src="js/jquery.steps.min.js"></script>
        <script src="js/countdown.min.js"></script>
        <script src="js/twitterfetcher.min.js"></script>
        <script src="js/spectragram.min.js"></script>
        <script src="js/smooth-scroll.min.js"></script>
        <script src="js/scripts.js"></script>
    </body>
</html>
